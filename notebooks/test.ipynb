{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from abc import abstractmethod\n",
    "from logging import LoggerAdapter\n",
    "from pathlib import Path\n",
    "from typing import ClassVar\n",
    "\n",
    "import numpy as np\n",
    "from kink import inject\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import mlflow\n",
    "from bunq_ynab_connect.classification.budget_category_encoder import (\n",
    "    BudgetCategoryEncoder,\n",
    ")\n",
    "from bunq_ynab_connect.classification.experiments.base_payment_classification_experiment import (  # noqa: E501\n",
    "    BasePaymentClassificationExperiment,\n",
    ")\n",
    "from bunq_ynab_connect.classification.feature_extractor_old import FeatureExtractor\n",
    "from bunq_ynab_connect.classification.feature_store import FeatureStore\n",
    "from bunq_ynab_connect.data.storage.abstract_storage import AbstractStorage\n",
    "from bunq_ynab_connect.data.storage.mongo_storage import MongoStorage\n",
    "from bunq_ynab_connect.models.matched_transaction import MatchedTransaction\n",
    "from mlflow.client import MlflowClient\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = MongoStorage()\n",
    "label_encoder = BudgetCategoryEncoder()\n",
    "budget_id = \"todo\"\n",
    "feature_store = FeatureStore()\n",
    "\n",
    "CLASSIFIERS: ClassVar[list[ClassifierMixin]] = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    # GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    MLPClassifier(max_iter=1000),\n",
    "    # ExplainableBoostingClassifier(),\n",
    "]\n",
    "\n",
    "N_FOLDS = 3\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> list[MatchedTransaction]:\n",
    "    \"\"\"Load the dataset.\n",
    "\n",
    "    - Load all matched transactions for the given budget\n",
    "    - Convert them to MatchedTransaction entities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        List of MatchedTransaction entities\n",
    "\n",
    "    \"\"\"\n",
    "    transactions = storage.find(\n",
    "        \"matched_transactions\",\n",
    "        [(\"ynab_transaction.budget_id\", \"eq\", budget_id)],\n",
    "    )\n",
    "    return storage.rows_to_entities(transactions, MatchedTransaction)\n",
    "\n",
    "\n",
    "def transactions_to_xy(\n",
    "    transactions: list[MatchedTransaction],\n",
    ") -> tuple[np.array, np.array]:\n",
    "    \"\"\"Convert a list of MatchedTransactions to X and y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        X: Array of bunq payments\n",
    "        y: Array of categories as integers\n",
    "\n",
    "    \"\"\"\n",
    "    X = np.array([t.bunq_payment.model_dump() for t in transactions])  # noqa: N806\n",
    "    y = np.array([t.ynab_transaction.model_dump() for t in transactions])\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Find the value counts in y\n",
    "    category_counts = pd.Series(y).value_counts()\n",
    "    top_categories = category_counts.nlargest(2).index\n",
    "\n",
    "    # Get the values that occur less than or equal to 3 times\n",
    "    categories_to_drop = category_counts[category_counts <= 5].index\n",
    "\n",
    "    # Get the indexes of these values in y\n",
    "    indexes_to_drop = np.where(np.isin(y, categories_to_drop))[0]\n",
    "\n",
    "    # Drop these indexes from X and y\n",
    "    X_filtered = np.delete(X, indexes_to_drop, axis=0)\n",
    "    y_filtered = np.delete(y, indexes_to_drop, axis=0)\n",
    "\n",
    "    print(f\"Dropped {len(indexes_to_drop)} items from X and y\")\n",
    "    \n",
    "    # drop all but the top 2 categories\n",
    "    indexes_to_drop = np.where(np.isin(y_filtered, top_categories, invert=True))[0]\n",
    "    X_filtered = np.delete(X_filtered, indexes_to_drop, axis=0)\n",
    "    y_filtered = np.delete(y_filtered, indexes_to_drop, axis=0)\n",
    "    \n",
    "    print(f\"Dropped {len(indexes_to_drop)} items from X and y\")\n",
    "    \n",
    "\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "\n",
    "def run() -> None:\n",
    "    \"\"\"Run the experiment.\n",
    "\n",
    "    - Load data\n",
    "    - Enable autolog\n",
    "        Skip logging of models, because this takes a lot of space\n",
    "    - Start run and _run\n",
    "    \"\"\"\n",
    "    transactions = load_data()\n",
    "    experiment_name = get_experiment_name()\n",
    "    if not len(transactions):\n",
    "        print(\"Skipping experiment %s, because no dataset was found\", experiment_name)\n",
    "        return\n",
    "    X, y = transactions_to_xy(transactions)  # noqa: N806\n",
    "    print(\"Running experiment %s\", experiment_name)\n",
    "    print(\"Dataset has size %s\", len(transactions))\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    mlflow.sklearn.autolog(log_models=False)\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.set_tag(\"budget\", budget_id)\n",
    "        parent_run_id = run.info.run_id\n",
    "        _run(X, y)\n",
    "\n",
    "\n",
    "def get_sample_values(y):\n",
    "    category_counts = pd.Series(y).value_counts()\n",
    "    category_counts_counts = category_counts.rename(\"n\").value_counts().reset_index()\n",
    "    category_counts_counts.columns = [\"count\", \"occurrences\"]\n",
    "    percentile_75_value = ceil(category_counts_counts[\"count\"].quantile(0.75))\n",
    "\n",
    "    # find all unique categories that occur more than the 75th percentile value\n",
    "    categories_to_undersample = category_counts[\n",
    "        category_counts > percentile_75_value\n",
    "    ].index\n",
    "\n",
    "    print(\n",
    "        f\"Categories to undersample to {percentile_75_value}: {categories_to_undersample}\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        c: percentile_75_value for c in categories_to_undersample\n",
    "    }\n",
    "\n",
    "\n",
    "def create_pipeline(\n",
    "    classifier: ClassifierMixin\n",
    ") -> Pipeline:\n",
    "    from bunq_ynab_connect.classification.feature_extractor_old import FeatureExtractor\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"feature_extractor\", feature_extractor),\n",
    "            (\n",
    "                \"undersample\",\n",
    "                RandomUnderSampler(\n",
    "                    sampling_strategy=get_sample_values\n",
    "                ),\n",
    "            ),\n",
    "            (\"oversample\", SMOTE(k_neighbors=3)),\n",
    "            (\"classifier\", classifier),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_experiment_name() -> str:\n",
    "    return \"NOTEBOOK TESTING\"\n",
    "\n",
    "\n",
    "@abstractmethod\n",
    "def _run(X: np.array, y: np.array) -> None:  # noqa: N803\n",
    "    for classifier in CLASSIFIERS:\n",
    "        with mlflow.start_run(run_name=classifier.__class__.__name__, nested=True):\n",
    "            run_classifier(classifier, X, y)\n",
    "\n",
    "\n",
    "def run_classifier(\n",
    "    model: ClassifierMixin,\n",
    "    X: np.ndarray,  # noqa: N803\n",
    "    y: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"Run the experiment for a single classifier.\n",
    "\n",
    "    - Create the pipeline\n",
    "    - Use Kfold\n",
    "    - Score and log the mean score\n",
    "\n",
    "    \"\"\"\n",
    "    mlflow.set_tag(\"classifier\", model.__class__.__name__)\n",
    "    classifier = create_pipeline(model)\n",
    "    k_fold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(\n",
    "        classifier,\n",
    "        X,\n",
    "        y,\n",
    "        cv=k_fold,\n",
    "        n_jobs=-1,\n",
    "        scoring=make_scorer(cohen_kappa_score),\n",
    "    )\n",
    "    mlflow.log_text(str(scores), \"scores.txt\")\n",
    "    avg_score = np.mean(scores)\n",
    "    mlflow.log_metric(\"cohen_kappa\", avg_score)\n",
    "    mlflow.sklearn.log_model(classifier, \"model\")\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = transactions_to_xy(load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = transactions_to_xy(load_data())\n",
    "# Find the value counts in y\n",
    "category_counts = pd.Series(y).value_counts()\n",
    "\n",
    "# Get the values that occur less than or equal to 3 times\n",
    "categories_to_drop = category_counts[category_counts <= 3].index\n",
    "\n",
    "# Get the indexes of these values in y\n",
    "indexes_to_drop = np.where(np.isin(y, categories_to_drop))[0]\n",
    "\n",
    "# Drop these indexes from X and y\n",
    "X_filtered = np.delete(X, indexes_to_drop, axis=0)\n",
    "y_filtered = np.delete(y, indexes_to_drop, axis=0)\n",
    "\n",
    "category_counts_counts = category_counts.rename(\"n\").value_counts().reset_index()\n",
    "category_counts_counts.columns = ['count', 'occurrences']\n",
    "percentile_75_value = ceil(category_counts_counts['count'].quantile(0.75))\n",
    "\n",
    "# find all unique categories that occur more than the 75th percentile value\n",
    "categories_to_undersample = category_counts[category_counts > percentile_75_value].index\n",
    "\n",
    "print(f\"Dropped {len(indexes_to_drop)} items from X and y\")\n",
    "print(f\"Categories to undersample to {percentile_75_value}: {categories_to_undersample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "X_extracted = FeatureExtractor().fit_transform(X_filtered)\n",
    "\n",
    "# undersample\n",
    "X_resampled, y_resampled = RandomUnderSampler(\n",
    "    sampling_strategy={\n",
    "        category: percentile_75_value for category in categories_to_undersample\n",
    "    }\n",
    ").fit_resample(X_extracted, y_filtered)\n",
    "\n",
    "# oversample\n",
    "X_resampled, y_resampled = SMOTE(k_neighbors=3).fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = storage.find(\n",
    "    \"matched_transactions\",\n",
    "    [(\"ynab_transaction.budget_id\", \"eq\", budget_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynab = [t for t in transactions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunq_ynab_connect.classification.feature_extractor import FeatureExtractor\n",
    "test = FeatureExtractor()\n",
    "\n",
    "test.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score, accuracy_score, f1_score, balanced_accuracy_score\n",
    "mlflow.set_experiment(\"Testing with sampling\")\n",
    "with mlflow.start_run():\n",
    "    X, y = transactions_to_xy(load_data())\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    mlflow.set_tag(\"classifier\", model.__class__.__name__)\n",
    "    classifier = create_pipeline(model)\n",
    "    k_fold = StratifiedKFold(\n",
    "        n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE\n",
    "    )\n",
    "    metrs = [\"accuracy\", \"f1\"]\n",
    "    scores = cross_validate(\n",
    "        classifier,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=k_fold,\n",
    "        n_jobs=-1,\n",
    "        scoring=metrs,\n",
    "    )\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    for metric in metrs:\n",
    "        mlflow.log_metric(f\"{metric}_validate\", np.mean(scores[f\"test_{metric}\"]))\n",
    "        fn = globals()[f\"{metric}_score\"]\n",
    "    mlflow.log_metric(\"accuracy_test\", accuracy_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"f1_test\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    mlflow.log_metric(\"cohen_kappa_test\", cohen_kappa_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"balanced_accuracy_test\", balanced_accuracy_score(y_test, y_pred))\n",
    "    mlflow.sklearn.log_model(classifier, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts_df = pd.Series(y_resampled).value_counts().reset_index()\n",
    "# rename count col\n",
    "category_counts_df.columns = ['category', 'cat_count']\n",
    "\n",
    "category_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = FeatureExtractor().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = storage.find(\n",
    "    \"matched_transactions\",\n",
    "    [(\"ynab_transaction.budget_id\", \"eq\", budget_id)],\n",
    ")\n",
    "transactions = storage.rows_to_entities(transactions, MatchedTransaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "y_trans = [t.ynab_transaction.model_dump() for t in transactions]\n",
    "# to frame\n",
    "y_trans_pd = pd.DataFrame(y_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the counts for each category\n",
    "category_counts = y_trans_pd['category_name'].value_counts()\n",
    "\n",
    "# Create the bar plot\n",
    "category_counts.plot(kind='bar', figsize=(12, 6))\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Counts for Each Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts_df = y_trans_pd['category_name'].value_counts().reset_index()\n",
    "# rename count col\n",
    "category_counts_df.columns = ['category', 'cat_count']\n",
    "\n",
    "count_of_counts = category_counts_df['cat_count'].value_counts().reset_index()\n",
    "count_of_counts.columns = ['count', 'occurrences']\n",
    "count_of_counts.sort_values('count')\n",
    "percentile_75_value = count_of_counts['count'].quantile(0.75)\n",
    "percentile_75_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
